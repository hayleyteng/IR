{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex,minibatch, compounding\n",
    "import re\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "from pprint import pprint\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import unidecode \n",
    "import string\n",
    "from datetime import datetime\n",
    "import docx\n",
    "from docx import Document\n",
    "from win32com import client as wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(path):\n",
    "    word = wc.Dispatch('Word.Application')\n",
    "    doc = word.Documents.Open(path)  \n",
    "    doc.SaveAs(path+'.docx', 12) \n",
    "    doc.SaveAs(path+'.txt',4)\n",
    "    d = Document(path+'.docx') \n",
    "    if d.tables!=[]:\n",
    "        print(d.tables)\n",
    "    doc.Close()\n",
    "    word.Quit()\n",
    "    return path+'.txt'\n",
    "\n",
    "# d=convert(r'D:\\UN\\N1906016.DOC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read(path):\n",
    "    path_txt=convert(path)\n",
    "    file=open(path_txt,'r')\n",
    "    file=file.read()\n",
    "    file2=unidecode.unidecode(file)\n",
    "    return file2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "infixes = nlp.Defaults.prefixes + ( r\"[-]~\",r'[/]~',r'\\.')\n",
    "infix_re = spacy.util.compile_infix_regex(infixes)\n",
    "def custom_tokenizer(nlp):\n",
    "    return Tokenizer(nlp.vocab, infix_finditer=infix_re.finditer)\n",
    "\n",
    "nlp.tokenizer = custom_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #改地址\n",
    "# # file=read('N1906016.DOC.txt')\n",
    "# # file2=read('N0057625.txt')\n",
    "# # file3=read('N0550446.txt')\n",
    "# # file4=read('N0147024.txt')\n",
    "# # file5=read('N0055335.txt')\n",
    "# file6=read(r'D:\\UN\\N0550488.txt')\n",
    "# # file7=read('N0550398.txt')\n",
    "# file8=read(r'D:\\UN\\N1821512.txt')\n",
    "# # doc=nlp(file)\n",
    "# # doc2=nlp(file2)\n",
    "# # doc3=nlp(file3)\n",
    "# # doc4=nlp(file4)\n",
    "# # doc5=nlp(file5)\n",
    "# doc6=nlp(file6)\n",
    "# # doc7=nlp(file7)\n",
    "# doc8=nlp(file8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def session(file):\n",
    "    doc=nlp(file)\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    pattern = [{},{'ORTH': 'session'}, {'IS_SPACE': True}]\n",
    "    matcher.add('session', None, pattern)\n",
    "    matches = matcher(doc)\n",
    "    spans = [(ent_id, doc[start : end-1]) for ent_id, start, end in matches]\n",
    "    return spans[0][1]\n",
    "\n",
    "def approval_date(file):\n",
    "    start=file.find('adopted by the General Assembly on')\n",
    "    if start!=-1:\n",
    "        end=file[start:].find('\\n')\n",
    "        item=file[start:start+end]\n",
    "        date=nlp(item)[6:]\n",
    "    else:\n",
    "        start=file.find('plenary meeting')\n",
    "        start2=file[start:].find('\\n')\n",
    "        item=file[start+start2:]\n",
    "        item=str(nlp(item)[:4]).strip()\n",
    "        date=nlp(item)\n",
    "    d = datetime.strptime(str(date), '%d %B %Y')\n",
    "    return d.strftime('%Y-%m-%d')\n",
    "\n",
    "def id_num(file):\n",
    "    doc=nlp(file)\n",
    "    pattern=re.compile(r'A/RES/\\d+/\\d+.+')\n",
    "    for match in re.finditer(pattern, doc.text):\n",
    "        start, end = match.span()\n",
    "        num=doc.text[start:end]\n",
    "    return num\n",
    "\n",
    "    \n",
    "def proponent_authority(file):\n",
    "    org=[]\n",
    "    start=file.find('on the report of')\n",
    "    if start!=-1:\n",
    "        end=file[start:].find('\\n')\n",
    "        item=file[start:start+end]\n",
    "        for entity in nlp(item).ents:\n",
    "            if entity.label_=='ORG':\n",
    "                org.append(entity.text)\n",
    "        return ' '.join(org)\n",
    "    else:\n",
    "        return 'plenary'   \n",
    "    \n",
    "def agenda_item(file):\n",
    "    start=file.find('Agenda item')\n",
    "    end=file[start:].find('\\n')\n",
    "    item=file[start:start+end]\n",
    "    return nlp(item)[2:]\n",
    "\n",
    "def title(file):\n",
    "    en=file.find('The General Assembly,')\n",
    "    doc=nlp(file[:en])\n",
    "    pattern=re.compile('\\d+/\\d+\\.')\n",
    "    match=re.search(pattern,doc.text)\n",
    "    start,end=match.span()\n",
    "    title=str(list(nlp(doc.text[end:]).sents)[0]).strip()\n",
    "    return title\n",
    "\n",
    "def annex(file):\n",
    "    file=file[:file.rfind('[')]\n",
    "    start=file.find('Annex')\n",
    "    if start==-1:\n",
    "        return None\n",
    "    else:\n",
    "        try:\n",
    "            tfile=file[start:]\n",
    "            pattern=re.compile('[\\t|\\n]1(\\s)(\\t)?[A-Z].+\\.')\n",
    "            match=re.search(pattern, tfile)\n",
    "            return nlp(tfile[:match.span()[0]])\n",
    "        except:\n",
    "            return nlp(file[start:])\n",
    "    \n",
    "def preamble(file,nlp=nlp):\n",
    "    file=file[file.find('The General Assembly,')+len('The General Assembly,'):]\n",
    "    pattern=re.compile('[A-Z][a-z]+')\n",
    "    match = pattern.search(file)\n",
    "    if str(file[match.span()[0]:match.span()[1]])[-1]=='s':\n",
    "        return None\n",
    "    else:\n",
    "        pattern=re.compile('[\\s\\S]*,(\\d+)?(\\s+)?\\n')\n",
    "        match=pattern.search(file)\n",
    "        return nlp(file[:match.span()[1]+1])\n",
    "    \n",
    "def operative(file,nlp=nlp):\n",
    "    file=file[file.find('The General Assembly,')+len('The General Assembly,'):]\n",
    "    file=file[:file.find('plenary meeting')]\n",
    "    file=file[:file.rfind('.')+1]\n",
    "\n",
    "    pattern=re.compile('[^\\d]1\\.(\\s|\\t)')\n",
    "    match=pattern.search(file)\n",
    "    sep2=match.span()[0]\n",
    "    if sep2!=-1:\n",
    "        doc=nlp(file[sep2-1:])\n",
    "    else:\n",
    "        pattern=re.compile('[\\s\\S]*,(\\d+)?(\\s+)?\\n')\n",
    "        match=pattern.search(file)\n",
    "        file=file[match.span()[1]:]\n",
    "        pattern2=re.compile('[A-Z][a-z]+s')\n",
    "        match=pattern.search(file)\n",
    "        doc=nlp(file[match.span()[0]:])\n",
    "    return doc\n",
    "\n",
    "    \n",
    "def closing_formula(file):\n",
    "    mark=file.find('plenary meeting')\n",
    "    start=file[:mark].rfind('\\n')\n",
    "    doc=nlp(file[start+1:])\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    pattern = [{'IS_DIGIT':True},{'IS_TITLE':True}, {'IS_DIGIT': True}]\n",
    "    matcher.add('date', None, pattern)\n",
    "    matches = matcher(doc)\n",
    "    spans = [(ent_id,doc[:end]) for ent_id, start, end in matches][0][1]\n",
    "    return spans\n",
    "    \n",
    "    \n",
    "             \n",
    "def footnote(file):   # ** unrecognize file 4\n",
    "    file=file[:file.rfind('[')]\n",
    "    file=file[file.find('plenary meeting'):]\n",
    "    doc=nlp(file)\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    pattern = [{'IS_DIGIT':True},{'IS_TITLE':True}, {'IS_DIGIT': True}]\n",
    "    matcher.add('date', None, pattern)\n",
    "    matches = matcher(doc)\n",
    "    spans = [(ent_id,doc [end+1:]) for ent_id, start, end in matches][0][1]\n",
    "    start=str(spans).find('Annex')\n",
    "    if start==-1:\n",
    "        return spans\n",
    "    else:\n",
    "        try:\n",
    "            pattern=re.compile('[\\t|\\n]1(\\s)(\\t)?[A-Z].+\\.') ##file8 \n",
    "            match=pattern.search(file)\n",
    "            return nlp(file[match.span()[0]:])\n",
    "        except:\n",
    "            return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reference(file): \n",
    "    reference=[]\n",
    "    doc=preamble(file)\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    pattern=[{'ORTH':{'IN':['resolution','resolutions','decision','decisions']}},\n",
    "             {'TEXT':{'REGEX':'(\\d+/\\d+\\s[A-Z]\\s)|([A-Z]+/\\d+)|(\\d+/\\d+)|(\\d+)'}},\n",
    "             {'ORTH':{'IN':['A','B','C','D','E']},'OP':'?'}]\n",
    "    matcher.add('citation',None, pattern)\n",
    "    matches = matcher(doc)\n",
    "    spans = [(start,(doc[start:end])) for ent_id, start, end in matches]\n",
    "    for (start,i) in spans:\n",
    "        if str(i[0])=='resolutions' or str(i[0])=='decisions':\n",
    "            pattern2=re.compile(',([^a-zA-Z]+)?\\n')\n",
    "            end=pattern2.search(doc[start+1:].text).span()[1]\n",
    "            doc2=(doc[start+1:].text)[:end]\n",
    "            an_re=doc2.find('resolution')\n",
    "            an_de=doc2.find('decision')\n",
    "            if an_re==-1 and an_de==-1:\n",
    "                pass\n",
    "            elif an_re==-1 or an_de==-1:\n",
    "                doc2=doc2[:max(an_re,an_de)]\n",
    "            else:\n",
    "                doc2=doc2[:min(an_re,an_de)]\n",
    "            cites=[]\n",
    "            found=re.findall(re.compile('(\\d+/\\d+\\s[A-Z]\\s)|([A-Z]+/\\d+)|(\\d+/\\d+)|(\\d+[^a-z\\n,]+[(])'),doc2)\n",
    "            for w in found:\n",
    "                c=[i for i in w if str(i)!=''] \n",
    "                if c[0][-1]=='(':\n",
    "                    c[0]=c[0][:-1]\n",
    "                cites.append(c[0])\n",
    "            cite=str(i[0])+' '+' '.join(cites)\n",
    "            reference.append(nlp(cite))\n",
    "        else:\n",
    "            reference.append(i)\n",
    "    return reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def places(file):   ##x 6 aff need trainig\n",
    "    pre_loc,op_loc=[],[]\n",
    "    pre=preamble(file)\n",
    "    for entity in pre.ents:\n",
    "        if entity.label_=='GPE':\n",
    "            print(entity.text)\n",
    "            if entity.text[0].isdigit() or entity.text=='States':\n",
    "                continue\n",
    "            text=entity.text.rstrip(string.digits)\n",
    "            pre_loc.append(text)\n",
    "    op=operative(file)\n",
    "    for entity in op.ents:\n",
    "        if entity.label_=='GPE':\n",
    "            if entity.text[0].isdigit() or entity.text=='States':\n",
    "                continue\n",
    "            text=entity.text.rstrip(string.digits)\n",
    "            op_loc.append(entity.text)\n",
    "    return pre_loc,op_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def org(file,nlp=nlp):   \n",
    "    pre_loc,op_loc=[],[]\n",
    "    pre=preamble(file,nlp)\n",
    "    for entity in pre.ents:\n",
    "        if entity.label_=='ORG':\n",
    "            if entity.text[0].isdigit() or entity.text=='States':\n",
    "                continue\n",
    "            text=entity.text.rstrip(string.digits)\n",
    "            pre_loc.append(text)\n",
    "    op=operative(file,nlp)\n",
    "    for entity in op.ents:\n",
    "        if entity.label_=='ORG':\n",
    "            if entity.text[0].isdigit() or entity.text=='States':\n",
    "                continue\n",
    "            text=entity.text.rstrip(string.digits)\n",
    "            op_loc.append(entity.text)\n",
    "    return pre_loc,op_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_date(file,nlp=nlp):   ##再看看\n",
    "    approval=approval_date(file)\n",
    "    doc=operative(file,nlp)\n",
    "    def get_date(doc=doc):\n",
    "        spans=[]\n",
    "        matcher = Matcher(nlp.vocab)\n",
    "        pattern = [{'IS_DIGIT':True,'OP':'?'},{'IS_TITLE':True,'OP':'?'},{'ORTH':'to','OP':'?'},\n",
    "                   {'IS_DIGIT':True},{'IS_TITLE':True}, {'IS_DIGIT': True}]\n",
    "        matcher.add('date', None, pattern)\n",
    "        matches = matcher(doc)\n",
    "        for ent_id, start, end in matches:\n",
    "            if end-start !=4:\n",
    "                if end-start==5 and str(doc[start]).isdigit() and len(str(doc[start]))<3:\n",
    "                    span=' '.join([str(doc[start]),str(doc[start+3]),str(doc[start+4])])\n",
    "                elif end-start==6:\n",
    "                    span=' '.join([str(doc[start]),str(doc[start+1]),str(doc[end-1])])\n",
    "                elif end-start==3:\n",
    "                    span=str(doc[start:end])\n",
    "                yan=nlp(span)\n",
    "                span=' '.join([str(yan[0]),str(yan[1]),str(yan[2])[:4]])    \n",
    "                if datetime.strptime(span, '%d %B %Y').strftime('%Y-%m-%d')>=approval:   \n",
    "                    spans.append(span)    \n",
    "        return spans\n",
    "        \n",
    "    def get_year(doc=doc):\n",
    "        spans=[]\n",
    "        matcher = Matcher(nlp.vocab)\n",
    "        pattern = [{'IS_TITLE':False}, {'TEXT': {'REGEX':'^\\d\\d\\d\\d'}}]\n",
    "        pattern2=[{'ORTH':'resolution','OP':'!'},{'TEXT': {'REGEX':'^\\d\\d\\d\\d'}}]\n",
    "        matcher.add('year', None, pattern)\n",
    "        matches = matcher(doc)\n",
    "        span=[(start,doc[start+1:end]) for ent_id, start, end in matches]\n",
    "        for i,(s,y) in enumerate(span):\n",
    "            try:\n",
    "                if not (int(str(y).strip())<datetime.strptime(approval, '%Y-%m-%d').year or 'resolution'in str(doc[s-1])):\n",
    "                    spans.append(span[i])\n",
    "            except:\n",
    "                pass\n",
    "        return [x[1] for x in spans]\n",
    "        \n",
    "        \n",
    "    return get_date(),get_year()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = [ \n",
    "    ('Recalling section XIV of its resolution 49/233 A of 23 December 1994,', { \n",
    "     'entities': [(52,68,'DATE')] \n",
    "    }), \n",
    "    ('Recalling also its decision 50/500 of 17 September 1996 on the financing of the United Nations Logistics Base at Brindisi, Italy, and its subsequent resolutions thereon, the latest of which was resolution 53/236 of 8 June 1999,', { \n",
    "     'entities': [ (38, 55, 'DATE'),(76,109,'ORG'),(215,226,'DATE')] \n",
    "    }), \n",
    "    ('Having considered the reports of the Secretary-General on the financing of the Logistics Base and the related reports of the Advisory Committee on Administrative and Budgetary Questions,2', { \n",
    "     'entities': [(33, 54, 'ORG'),(75,93,'ORG'),(121,185,'ORG')] \n",
    "    }), \n",
    "    ('Recalling its resolutions 54/196 of 22 December 1999, 55/186 and 55/213 of 20 December 2000 and 55/245 A of 21 March 2001, and decision 1/1 of the Preparatory Committee for the International Conference on Financing for Development,1', { \n",
    "     'entities': [(36,52,'DATE'),(108,121,'DATE'),(143,168,'ORG')] \n",
    "    }), \n",
    "    ('Having considered the reports of the Secretary-General on the financing of the United Nations Observer Mission in Georgia1 and the related report of the Advisory Committee on Administrative and Budgetary Questions,2 ',{\n",
    "     'entities':[(33,54,'ORG'),(75,110,'ORG'),(149,213,'ORG')]\n",
    "    }),\n",
    "    ('Recalling Security Council resolution 854 (1993) of 6 August 1993,',{\n",
    "    'entities':[(10,26,'ORG'),(52,65,'DATE')]\n",
    "    }),\n",
    "    ('the Council established the United Nations Observer Mission in Georgia',{\n",
    "    'entities':[(24,59,'ORG')]\n",
    "    }),\n",
    "    ('Having considered the reports of the Secretary-General on the budget performance of the support account for peacekeeping operations for the period from 1 July 2016 to 30 June 2017,1 on the budget for the support account for peacekeeping operations',{\n",
    "    'entities':[(33,54,'ORG'),(152,163,'DATE'),(167,179,'DATE'),]\n",
    "    }),\n",
    "    ('the report of the Independent Audit Advisory Committee on the proposed budget of the Office of Internal Oversight Services under the support account and the related report of the Advisory Committee on Administrative and Budgetary Questions,5',{\n",
    "    'entities':[(14,54,'ORG'),(81,122,'ORG'),(175,240,'ORG')] \n",
    "    })\n",
    "        \n",
    "\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(model=None, output_dir=None, n_iter=150,labels=None):\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model) \n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  \n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner)\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "    \n",
    "    if labels is not None:\n",
    "        for label in labels:\n",
    "            ner.add_label(label)\n",
    "\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        optimizer = nlp.resume_training()\n",
    "    \n",
    "    move_names = list(ner.move_names)\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        sizes = compounding(1.0, 4.0, 1.001)\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            batches = minibatch(TRAIN_DATA, size=sizes)\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "\n",
    "    for text, _ in TRAIN_DATA:\n",
    "        doc = nlp(text)\n",
    "        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "        print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        for text, _ in TRAIN_DATA:\n",
    "            doc = nlp2(text)\n",
    "            print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "            print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "    return nlp2\n",
    "\n",
    "# nlp2=main(\"en_core_web_sm\",r'D:\\UN')\n",
    "# nlp2=main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def op_to_sentence(op):\n",
    "\n",
    "    sents=op.text.split('\\n')\n",
    "    sents=[x for x in sents if x is not None and x !=' 'and x!='']\n",
    "    sentences=[]\n",
    "    for sent in sents:\n",
    "        pattern=re.compile('[A-Z][a-z]+')\n",
    "        match=pattern.search(sent)\n",
    "        sentence=sent[match.span()[0]:]\n",
    "        pattern2=re.compile('\\s\\((.+)\\)')\n",
    "        match2=pattern2.search(sentence)\n",
    "        if match2 is not None:\n",
    "            sentence=sentence[:match2.span()[0]]+sentence[match2.span()[1]:]\n",
    "        sentence=purify(sentence)\n",
    "        sentence=purify2(sentence)\n",
    "        sentences.append(sentence)\n",
    "\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purify(sentence):   ##去掉上角标, 去掉resolution, 去掉数字中逗号,去掉in accordance with等\n",
    "    sentence=sentence.replace('also ','').replace('Also ','').replace('Further ','').replace('further ','')   \n",
    "    \n",
    "    begin=sentence.find('in accordance with')\n",
    "    if begin!=-1:\n",
    "        comma=sentence[begin:].find(',')\n",
    "        if comma!=-1:\n",
    "            sentence=sentence[:begin]+sentence[begin+comma+1:]\n",
    "        else:\n",
    "            sentence=sentence[:begin]\n",
    "            \n",
    "    begin=sentence.find('in order to')\n",
    "    if begin!=-1:\n",
    "        comma=sentence[begin:].find(',')\n",
    "        if comma!=-1:\n",
    "            sentence=sentence[:begin]+sentence[begin+comma+1:]\n",
    "        else:\n",
    "            sentence=sentence[:begin]\n",
    "    \n",
    "    begin=sentence.find('commensurate with')\n",
    "    if begin!=-1:\n",
    "        comma=sentence[begin:].find(',')\n",
    "        if comma!=-1:\n",
    "            sentence=sentence[:begin]+sentence[begin+comma+1:]\n",
    "        else:\n",
    "            sentence=sentence[:begin]\n",
    "    \n",
    "    begin=sentence.find('inclusive of')\n",
    "    if begin!=-1:\n",
    "        comma=sentence[begin:].find(',')\n",
    "        if comma!=-1:\n",
    "            sentence=sentence[:begin]+sentence[begin+comma+1:]\n",
    "        else:\n",
    "            sentence=sentence[:begin]\n",
    "    \n",
    "    begin=sentence.find('including')\n",
    "    if begin!=-1:\n",
    "        comma=sentence[begin:].find(',')\n",
    "        if comma!=-1:\n",
    "            sentence=sentence[:begin]+sentence[begin+comma+1:]\n",
    "        else:\n",
    "            sentence=sentence[:begin]\n",
    "            \n",
    "    begin=sentence.find('as provided for')\n",
    "    if begin!=-1:\n",
    "        comma=sentence[begin:].find(',')\n",
    "        if comma!=-1:\n",
    "            sentence=sentence[:begin]+sentence[begin+comma+1:]\n",
    "        else:\n",
    "            sentence=sentence[:begin]\n",
    "            \n",
    "    begin=sentence.find('bearing in mind')\n",
    "    if begin!=-1:\n",
    "        comma=sentence[begin:].find(',')\n",
    "        if comma!=-1:\n",
    "            sentence=sentence[:begin]+sentence[begin+comma+1:]\n",
    "        else:\n",
    "            sentence=sentence[:begin]\n",
    "            \n",
    "    begin=sentence.find('in respect of')\n",
    "    if begin!=-1:\n",
    "        comma=sentence[begin:].find(',')\n",
    "        if comma!=-1:\n",
    "            if len(nlp(sentence[begin:begin+comma]))<12:\n",
    "                sentence=sentence[:begin]+sentence[begin+comma+1:]\n",
    "        else:\n",
    "            if len(nlp(sentence[begin]))<12:\n",
    "                sentence=sentence[:begin]\n",
    "    \n",
    "    begin=sentence.find('as well as')\n",
    "    if begin!=-1:\n",
    "        comma=sentence[begin:].find(',')\n",
    "        if comma!=-1:\n",
    "            sentence=sentence[:begin]+sentence[begin+comma+1:]\n",
    "        else:\n",
    "            sentence=sentence[:begin]\n",
    "            \n",
    "    begin=sentence.find('as set out')\n",
    "    if begin!=-1:\n",
    "        comma=sentence[begin:].find(',')\n",
    "        if comma!=-1:\n",
    "            sentence=sentence[:begin]+sentence[begin+comma+1:]\n",
    "        else:\n",
    "            sentence=sentence[:begin]\n",
    "            \n",
    "    begin=sentence.find('representing')\n",
    "    if begin!=-1:\n",
    "        comma=sentence[begin:].find(',')\n",
    "        if comma!=-1:\n",
    "            sentence=sentence[:begin]+sentence[begin+comma+1:]\n",
    "        else:\n",
    "            sentence=sentence[:begin]\n",
    "            \n",
    "    begin=sentence.find('referred to')\n",
    "    if begin!=-1:\n",
    "        comma=sentence[begin:].find(',')\n",
    "        if comma!=-1:\n",
    "            sentence=sentence[:begin]+sentence[begin+comma+1:]\n",
    "        else:\n",
    "            sentence=sentence[:begin]\n",
    "    \n",
    "    index=sentence.rfind(';')\n",
    "    if index!=-1:\n",
    "        sentence=sentence[:index+1]\n",
    "        \n",
    "    pattern=re.compile('\\d+,\\d+(,)?(\\d+)?(,)?(\\d+)?')\n",
    "    for m in pattern.finditer(sentence):\n",
    "        mm = m.group()\n",
    "        sentence = sentence.replace(mm,mm.replace(',',''))\n",
    "\n",
    "    sentence=nlp(sentence)\n",
    "    \n",
    "\n",
    "\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    pattern=[{'ORTH':{'IN':['of','in']}},\n",
    "             {'ORTH':{'IN':['its','General']}},\n",
    "             {'ORTH':'Assembly','OP':'?'},\n",
    "             {'ORTH':{'IN':['resolution','resolutions']}}]\n",
    "    matcher.add('refer',None, pattern)\n",
    "    matches = matcher(sentence)\n",
    "    if matches !=[]:\n",
    "        record=[]\n",
    "        for _,start,end in matches:\n",
    "            for item in sentence[end:]:\n",
    "                if item.is_punct:\n",
    "                    record.append((end,item.i))\n",
    "                    break\n",
    "        sent=[str(t) for t in sentence]\n",
    "        for t in reversed(record):\n",
    "            del sent[t[0]:t[1]]\n",
    "        sentence=' '.join(sent)\n",
    "        return nlp(sentence)\n",
    "    \n",
    "    return sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purify2(sentence): # sentence should be nlp format  #去掉 para 去掉 from to 去掉 插入语 合并18.7\n",
    "    matcher=Matcher(nlp.vocab)\n",
    "    pattern=[{'ORTH':{'IN':['paragraph','paragraphs']}},{'IS_DIGIT':True},\n",
    "            {'ORTH':'and'},{'IS_DIGIT':True,'OP':'?'}]\n",
    "    matcher.add('para',None, pattern)\n",
    "    matches = matcher(sentence)\n",
    "    if matches!=[]:\n",
    "        idx,start,end=matches[0]\n",
    "        temp=str(sentence[:start+1])+' '+str(sentence[end:])\n",
    "        sentence=nlp(temp)\n",
    "    \n",
    "    matcher=Matcher(nlp.vocab)\n",
    "    pattern2 = [{'ORTH':'from'},{'IS_DIGIT':True,'OP':'?'},{'IS_TITLE':True,'OP':'?'},{'IS_DIGIT':True,'OP':'?'},{'ORTH':'to'},\n",
    "               {'IS_DIGIT':True,'OP':'?'},{'IS_TITLE':True}, {'IS_DIGIT': True}]\n",
    "    matcher.add('date', None, pattern2)\n",
    "    matches=matcher(sentence)\n",
    "    if matches!=[]:\n",
    "        idx,start,end=matches[0]\n",
    "        temp=str(sentence[:start])+' '+str(sentence[end:])\n",
    "        sentence=nlp(temp)\n",
    "\n",
    "    matcher=Matcher(nlp.vocab)\n",
    "    pattern6 = [{'ORTH':'at'},{'IS_DIGIT':True,'OP':'?'},{'IS_TITLE':True},{'IS_DIGIT':True}]\n",
    "    matcher.add('date', None, pattern6)\n",
    "    matches=matcher(sentence)\n",
    "    if matches!=[]:\n",
    "        idx,start,end=matches[0]\n",
    "        temp=str(sentence[:start])+' '+str(sentence[end:])\n",
    "        sentence=nlp(temp)\n",
    "        \n",
    "    record=[]\n",
    "    matcher=Matcher(nlp.vocab)\n",
    "    pattern3=[{'ORTH':','},{'OP':'+'},{'ORTH':','}]\n",
    "    matcher.add('charuyu',None,pattern3)\n",
    "    matches=matcher(sentence)\n",
    "    if matches!=[]:\n",
    "        for _,start,end in matches:\n",
    "            if end-start<=5:\n",
    "                record.append((start+1,end))\n",
    "        if record!=[]:\n",
    "            sent=[str(t) for t in sentence]\n",
    "            for t in reversed(record):\n",
    "                del sent[t[0]:t[1]]\n",
    "            sentence=nlp(' '.join(sent))\n",
    "    \n",
    "    record=[]\n",
    "    matcher=Matcher(nlp.vocab)\n",
    "    pattern5=[{'ORTH':','},{'IS_DIGIT':True},{'ORTH':'dollars'}]\n",
    "    matcher.add('charuyu',None,pattern5)\n",
    "    matches=matcher(sentence)\n",
    "    if matches !=[]:\n",
    "        record=[]\n",
    "        for _,start,end in matches:\n",
    "            for item in sentence[end:]:\n",
    "                if item.is_punct:\n",
    "                    record.append((start+1,item.i))\n",
    "                    break\n",
    "        sent=[str(t) for t in sentence]\n",
    "        for t in reversed(record):\n",
    "            del sent[t[0]:t[1]]\n",
    "        sentence=nlp(' '.join(sent))\n",
    "    \n",
    "            \n",
    "    matcher=Matcher(nlp.vocab)\n",
    "    pattern4=[{'IS_DIGIT':True},{'ORTH':'.'},{'IS_DIGIT':True}]\n",
    "    matcher.add('num',None,pattern4)\n",
    "    matches=matcher(sentence)\n",
    "    if matches!=[]:\n",
    "        for _, start, end in matches:\n",
    "            sentence[start:end].merge()\n",
    "    \n",
    "    return sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_test(sents):\n",
    "    result=[]\n",
    "    for sent in sents:\n",
    "        childs=[]\n",
    "        grands=[]\n",
    "        extragrands=[]\n",
    "        sons=[]\n",
    "        babies=[]\n",
    "        roots = [token for token in sent if token.head == token]\n",
    "        for token in sent:\n",
    "            if str(token)=='no':\n",
    "                roots.append(token)\n",
    "            if str(token)=='not':\n",
    "                babies.append(token)\n",
    "\n",
    "        for root in roots:\n",
    "            for item in root.children:\n",
    "                if item.dep_!='punct' and item.pos_!='DET' and item.dep_!='aux' and item.dep_!='mark' and item.dep_!='appos':\n",
    "                    childs.append(item)\n",
    "\n",
    "            for child in childs:\n",
    "                for item in child.children:\n",
    "                    if item.dep_!='punct'and item.pos_!='DET'and item.dep_!='aux'and item.dep_!='mark':\n",
    "                        if str(item)=='comprising':\n",
    "                            pass\n",
    "                        else:\n",
    "                            grands.append(item)\n",
    "\n",
    "            for grand in grands:\n",
    "                for item in grand.children:\n",
    "                    if item.pos_=='PROPN' or item.pos_=='NOUN'  or item.dep_=='cc'or item.dep_=='ccomp' or item.pos_=='VERB' or item.pos_=='NUM':\n",
    "                        if str(item)=='period'or str(item)=='amount':\n",
    "                            grands.remove(grand)\n",
    "                        elif item.dep_=='advcl':\n",
    "                            pass\n",
    "                        else:\n",
    "                            extragrands.append(item)\n",
    "                    elif item.pos_=='ADP':\n",
    "                        for word in item.children:\n",
    "                            if word.dep_=='pobj':\n",
    "                                if sent[word.i-1].pos_=='NUM':\n",
    "                                    sons.append(sent[word.i-1])\n",
    "                                    sons.append(word)\n",
    "                                if sent[word.i-1].dep_=='compound' and sent[word.i-2].dep_=='compound':\n",
    "                                    sons.append(sent[word.i-1])\n",
    "                                    sons.append(sent[word.i-2])\n",
    "                                    sons.append(word)\n",
    "                                    extragrands.append(item)\n",
    "                                try:\n",
    "                                    temp=word.nbor().nbor()\n",
    "                                    if word.is_ancestor(temp) and temp.pos_!='DET':\n",
    "                                        sons.append(temp)\n",
    "                                        if temp in temp.head.lefts:\n",
    "                                            sons.append(temp.head)\n",
    "                                        else:\n",
    "                                            sons.append(temp.head.head)\n",
    "                                        for wd in temp.children:\n",
    "                                            if wd.dep_=='conj'or wd.dep_=='dobj':\n",
    "                                                babies.append(wd)\n",
    "                                except:\n",
    "                                    pass\n",
    "\n",
    "                    elif str(item)=='of':\n",
    "                        for word in item.children:\n",
    "                            if word.pos_=='NOUN':\n",
    "                                extragrands.append(item)\n",
    "                                sons.append(word)\n",
    "                                for wd in word.children:\n",
    "                                    if wd.dep_=='conj':\n",
    "                                        sons.append(wd)\n",
    "                    elif str(item)=='for':\n",
    "                        for word in item.children:\n",
    "                            if word.pos_=='NUM':\n",
    "                                extragrands.append(item)\n",
    "                                sons.append(word)\n",
    "\n",
    "            for extragrand in extragrands:\n",
    "                for item in extragrand.children:\n",
    "                    if item.pos_=='PROPN' or item.pos_=='NOUN' or item.dep_=='nummod' or item.pos_=='NUM':\n",
    "                        sons.append(item)\n",
    "                    if item.pos_=='VERB':\n",
    "                        for word in item.children:\n",
    "                            if word.pos_=='NOUN':\n",
    "                                sons.append(item)\n",
    "                                babies.append(word)\n",
    "                    if item.dep_=='amod':\n",
    "                        if item in list(extragrand.lefts):\n",
    "                            sons.append(item)\n",
    "                    if str(item)=='of':\n",
    "                        for word in item.children:\n",
    "                            if word.pos_=='NOUN'and str(word)!='dollars':\n",
    "                                sons.append(item)\n",
    "                                babies.append(word)\n",
    "                                for wd in word.children:\n",
    "                                    if wd.dep_=='conj':\n",
    "                                        babies.append(wd)\n",
    "                    if str(item)=='on':\n",
    "                        for word in item.children:\n",
    "                            if word.pos_=='NOUN':\n",
    "                                sons.append(item)\n",
    "                                babies.append(word)\n",
    "                                for wd in word.children:\n",
    "                                    if wd.dep=='conj' or wd.dep_=='amod' and wd in list(word.lefts):\n",
    "                                        babies.append(wd)\n",
    "\n",
    "\n",
    "        dic={}\n",
    "        for t in roots+childs+grands+extragrands+sons+babies:\n",
    "            dic[t]=t.i\n",
    "\n",
    "        trunk=sorted(dic.items(), key=lambda x: x[1])\n",
    "        extracted=[t[0] for t in trunk]\n",
    "    \n",
    "        result.append(extracted)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"439e702f0f9241a499085bd5487601df-0\" class=\"displacy\" width=\"1450\" height=\"347.0\" direction=\"ltr\" style=\"max-width: none; height: 347.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Decides</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"120\">that</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"120\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"190\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"190\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"260\">increase</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"260\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"330\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"330\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">37400</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"470\">dollars</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"470\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"540\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"540\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"610\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"610\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"680\">estimated</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"680\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">staff</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"820\">assessment</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"820\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"890\">income</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"890\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"960\">854900</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"960\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1030\">dollars</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1030\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">referred</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1170\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1170\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1240\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1240\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1310\">paragraphs</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1310\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1380\">above;</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1380\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-439e702f0f9241a499085bd5487601df-0-0\" stroke-width=\"2px\" d=\"M70,212.0 C70,177.0 95.0,177.0 95.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-439e702f0f9241a499085bd5487601df-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M95.0,214.0 L103.0,202.0 87.0,202.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-439e702f0f9241a499085bd5487601df-0-1\" stroke-width=\"2px\" d=\"M210,212.0 C210,177.0 235.0,177.0 235.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-439e702f0f9241a499085bd5487601df-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M210,214.0 L202,202.0 218,202.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-439e702f0f9241a499085bd5487601df-0-2\" stroke-width=\"2px\" d=\"M70,212.0 C70,107.0 245.0,107.0 245.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-439e702f0f9241a499085bd5487601df-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245.0,214.0 L253.0,202.0 237.0,202.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-439e702f0f9241a499085bd5487601df-0-3\" stroke-width=\"2px\" d=\"M280,212.0 C280,177.0 305.0,177.0 305.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-439e702f0f9241a499085bd5487601df-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M305.0,214.0 L313.0,202.0 297.0,202.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-439e702f0f9241a499085bd5487601df-0-4\" stroke-width=\"2px\" d=\"M420,212.0 C420,177.0 445.0,177.0 445.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-439e702f0f9241a499085bd5487601df-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,214.0 L412,202.0 428,202.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-439e702f0f9241a499085bd5487601df-0-5\" stroke-width=\"2px\" d=\"M350,212.0 C350,142.0 450.0,142.0 450.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-439e702f0f9241a499085bd5487601df-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M450.0,214.0 L458.0,202.0 442.0,202.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-439e702f0f9241a499085bd5487601df-0-6\" stroke-width=\"2px\" d=\"M280,212.0 C280,72.0 530.0,72.0 530.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-439e702f0f9241a499085bd5487601df-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M530.0,214.0 L538.0,202.0 522.0,202.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-439e702f0f9241a499085bd5487601df-0-7\" stroke-width=\"2px\" d=\"M630,212.0 C630,72.0 880.0,72.0 880.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-439e702f0f9241a499085bd5487601df-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M630,214.0 L622,202.0 638,202.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-439e702f0f9241a499085bd5487601df-0-8\" stroke-width=\"2px\" d=\"M700,212.0 C700,107.0 875.0,107.0 875.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-439e702f0f9241a499085bd5487601df-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M700,214.0 L692,202.0 708,202.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-439e702f0f9241a499085bd5487601df-0-9\" stroke-width=\"2px\" d=\"M770,212.0 C770,177.0 795.0,177.0 795.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-439e702f0f9241a499085bd5487601df-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,214.0 L762,202.0 778,202.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-439e702f0f9241a499085bd5487601df-0-10\" stroke-width=\"2px\" d=\"M840,212.0 C840,177.0 865.0,177.0 865.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-439e702f0f9241a499085bd5487601df-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M840,214.0 L832,202.0 848,202.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-439e702f0f9241a499085bd5487601df-0-11\" stroke-width=\"2px\" d=\"M560,212.0 C560,37.0 885.0,37.0 885.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-439e702f0f9241a499085bd5487601df-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M885.0,214.0 L893.0,202.0 877.0,202.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-439e702f0f9241a499085bd5487601df-0-12\" stroke-width=\"2px\" d=\"M980,212.0 C980,177.0 1005.0,177.0 1005.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-439e702f0f9241a499085bd5487601df-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M980,214.0 L972,202.0 988,202.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-439e702f0f9241a499085bd5487601df-0-13\" stroke-width=\"2px\" d=\"M910,212.0 C910,142.0 1010.0,142.0 1010.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-439e702f0f9241a499085bd5487601df-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">appos</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1010.0,214.0 L1018.0,202.0 1002.0,202.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-439e702f0f9241a499085bd5487601df-0-14\" stroke-width=\"2px\" d=\"M280,212.0 C280,2.0 1100.0,2.0 1100.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-439e702f0f9241a499085bd5487601df-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1100.0,214.0 L1108.0,202.0 1092.0,202.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-439e702f0f9241a499085bd5487601df-0-15\" stroke-width=\"2px\" d=\"M1120,212.0 C1120,177.0 1145.0,177.0 1145.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-439e702f0f9241a499085bd5487601df-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1145.0,214.0 L1153.0,202.0 1137.0,202.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-439e702f0f9241a499085bd5487601df-0-16\" stroke-width=\"2px\" d=\"M1120,212.0 C1120,142.0 1220.0,142.0 1220.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-439e702f0f9241a499085bd5487601df-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1220.0,214.0 L1228.0,202.0 1212.0,202.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-439e702f0f9241a499085bd5487601df-0-17\" stroke-width=\"2px\" d=\"M1260,212.0 C1260,177.0 1285.0,177.0 1285.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-439e702f0f9241a499085bd5487601df-0-17\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1285.0,214.0 L1293.0,202.0 1277.0,202.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-439e702f0f9241a499085bd5487601df-0-18\" stroke-width=\"2px\" d=\"M1120,212.0 C1120,72.0 1370.0,72.0 1370.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-439e702f0f9241a499085bd5487601df-0-18\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1370.0,214.0 L1378.0,202.0 1362.0,202.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# displacy.render(sents[-5], style='dep', jupyter=True, options={'distance': 70})\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roots: [Decides]\n",
      "childs: [added]\n",
      "grands: [increase, be, to, from]\n",
      "666\n",
      "777\n",
      "extragrands: [in, credits]\n",
      "sons: [income]\n",
      "daugh: [37400, dollars, assessment, staff, income]\n",
      "babies: []\n",
      "[Decides, increase, 37400, dollars, in, staff, assessment, income, be, added, to, credits]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# childs=[]\n",
    "# grands=[]\n",
    "# extragrands=[]\n",
    "# sons=[]\n",
    "# daugh=[]\n",
    "# babies=[]\n",
    "# sent=sents[-5]\n",
    "# roots = [token for token in sent if token.head == token]\n",
    "\n",
    "# for token in sent:\n",
    "#     if str(token)=='no':\n",
    "#         roots.append(token)\n",
    "#     if str(token)=='not':\n",
    "#         babies.append(token)\n",
    "# print(\"roots:\",roots)\n",
    "        \n",
    "# for root in roots:\n",
    "#     for item in root.children:\n",
    "#         if item.dep_!='punct' and item.pos_!='DET' and item.dep_!='aux' and item.dep_!='mark' and item.dep_!='appos':\n",
    "#             childs.append(item)\n",
    "#     print(\"childs:\",childs)\n",
    "    \n",
    "#     for child in childs:\n",
    "#         for item in child.children:\n",
    "#             if item.dep_!='punct'and item.pos_!='DET'and item.dep_!='aux'and item.dep_!='mark':\n",
    "#                 if str(item)=='comprising':\n",
    "#                     pass\n",
    "#                 else:\n",
    "#                     grands.append(item)\n",
    "#     print(\"grands:\",grands)\n",
    "    \n",
    "#     for grand in grands:\n",
    "#         for item in grand.children:\n",
    "#             if item.pos_=='PROPN' or item.pos_=='NOUN'  or item.dep_=='cc' or item.dep_=='ccomp' or item.pos_=='VERB' or item.pos_=='NUM':\n",
    "#                 if str(item)=='period' or str(item)=='amount' :\n",
    "#                     grands.remove(grand)\n",
    "#                 elif item.dep_=='advcl':\n",
    "#                     pass\n",
    "#                 elif item.pos_=='VERB' and item in list(grand.lefts):\n",
    "#                     pass\n",
    "\n",
    "#                 else:\n",
    "#                     extragrands.append(item)\n",
    "                    \n",
    "#             elif item.pos_=='ADP':\n",
    "#                 for word in item.children:\n",
    "#                     if word.dep_=='pobj':   \n",
    "#                         if sent[word.i-1].pos_=='NUM':\n",
    "#                             print(666)\n",
    "#                             daugh.append(sent[word.i-1])\n",
    "#                             daugh.append(word)\n",
    "#                         if sent[word.i-1].dep_=='compound' and sent[word.i-2].dep_=='compound':\n",
    "#                             print(777)\n",
    "#                             daugh.append(sent[word.i-1])\n",
    "#                             daugh.append(sent[word.i-2])\n",
    "#                             daugh.append(word)\n",
    "#                             extragrands.append(item)\n",
    "#                         temp=word.nbor().nbor()\n",
    "# #                         if word.is_ancestor(temp) and temp.pos_=='NUM':\n",
    "# #                             sons.append(temp)\n",
    "# #                             sons.append(temp.head)\n",
    "#                         if word.is_ancestor(temp) and temp.pos_!='DET':\n",
    "#                             print(888)\n",
    "#                             print(word,temp)\n",
    "#                             daugh.append(temp)\n",
    "#                             if temp in temp.head.lefts:\n",
    "#                                 daugh.append(temp.head)\n",
    "#                             else:\n",
    "#                                 daugh.append(temp.head.head)\n",
    "#                             for wd in temp.children:\n",
    "#                                 if wd.dep_=='conj' or wd.dep_=='dobj':\n",
    "#                                     daugh.append(wd)\n",
    "\n",
    "#             elif str(item)=='of':\n",
    "#                 for word in item.children:\n",
    "#                     if word.pos_=='NOUN':\n",
    "#                         extragrands.append(item)\n",
    "#                         sons.append(word)\n",
    "#                         for wd in word.children:\n",
    "#                             if wd.dep_=='conj':\n",
    "#                                 sons.append(wd)\n",
    "#             elif str(item)=='for':\n",
    "#                 for word in item.children:\n",
    "#                     if word.pos_=='NUM':\n",
    "#                         extragrands.append(item)\n",
    "#                         sons.append(word)\n",
    "            \n",
    "            \n",
    "#     print(\"extragrands:\",extragrands)\n",
    "    \n",
    "#     for extragrand in extragrands:\n",
    "#         for item in extragrand.children:\n",
    "#             if item.pos_=='PROPN' or item.pos_=='NOUN' or item.dep_=='nummod' or item.dep_=='pobj' or item.pos_=='NUM':\n",
    "#                 sons.append(item)\n",
    "#             if item.pos_=='VERB':\n",
    "#                 for word in item.children:\n",
    "#                     if word.pos_=='NOUN':\n",
    "#                         sons.append(item)\n",
    "#                         babies.append(word)\n",
    "#             if item.dep_=='amod':\n",
    "#                 print(list(extragrand.lefts))\n",
    "#                 if item in list(extragrand.lefts):\n",
    "#                     sons.append(item)\n",
    "#             if str(item)=='of':\n",
    "#                 for word in item.children:\n",
    "#                     if word.pos_=='NOUN'and str(word)!='dollars':\n",
    "#                         sons.append(item)\n",
    "#                         babies.append(word)\n",
    "#                         for wd in word.children:\n",
    "#                             if wd.dep_=='conj':\n",
    "#                                 babies.append(wd)\n",
    "#             if str(item)=='on':\n",
    "#                 for word in item.children:\n",
    "#                     if word.pos_=='NOUN':\n",
    "#                         sons.append(item)\n",
    "#                         babies.append(word)\n",
    "#                         for wd in word.children:\n",
    "#                             if wd.dep=='conj' or wd.dep_=='amod' and wd in list(word.lefts):\n",
    "#                                 babies.append(wd)\n",
    "                \n",
    "#     print(\"sons:\",sons)\n",
    "#     print(\"daugh:\",daugh)\n",
    "#     print(\"babies:\",babies)\n",
    "\n",
    "# dic={}\n",
    "# for t in roots+childs+grands+extragrands+sons+babies+daugh:\n",
    "#     dic[t]=t.i\n",
    "\n",
    "# trunk=sorted(dic.items(), key=lambda x: x[1])\n",
    "\n",
    "# print([t[0] for t in trunk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expresses \t NOUN \t amod \t concern\n",
      "concern \t NOUN \t ROOT \t concern\n",
      "at \t ADP \t prep \t concern\n",
      "the \t DET \t det \t delay\n",
      "delay \t NOUN \t pobj \t at\n",
      "experienced \t VERB \t acl \t delay\n",
      "by \t ADP \t agent \t experienced\n",
      "the \t DET \t det \t Secretary-General\n",
      "Secretary-General \t PROPN \t pobj \t by\n",
      "in \t ADP \t prep \t experienced\n",
      "deploying \t VERB \t pcomp \t in\n",
      "and \t CCONJ \t cc \t deploying\n",
      "providing \t VERB \t conj \t deploying\n",
      "adequate \t ADJ \t amod \t resources\n",
      "resources \t NOUN \t dobj \t providing\n",
      "to \t ADP \t prep \t providing\n",
      "some \t DET \t det \t missions\n",
      "recent \t ADJ \t amod \t missions\n",
      "peacekeeping \t NOUN \t compound \t missions\n",
      "missions \t NOUN \t pobj \t to\n",
      ", \t PUNCT \t punct \t concern\n",
      "in \t ADP \t prep \t those\n",
      "particular \t ADJ \t amod \t in\n",
      "those \t DET \t appos \t concern\n",
      "in \t ADP \t prep \t those\n",
      "Africa \t PROPN \t pobj \t in\n",
      "; \t PUNCT \t punct \t concern\n"
     ]
    }
   ],
   "source": [
    "# for token in sents[3]:\n",
    "#     print(token,'\\t',token.pos_,'\\t',token.dep_,'\\t',token.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
